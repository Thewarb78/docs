sudo apt update
sudo apt upgrade -y

sudo apt install -y ca-certificates curl gnupg lsb-release

sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo mkdir -p /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
echo \
  "deb [arch=$(dpkg --print-architecture) signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

sudo apt update
sudo apt install -y docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

sudo docker --version
sudo docker run hello-world

test

####

{
  "proxies": {
    "default": {
      "httpProxy": "http://your.proxy.server:port",
      "httpsProxy": "http://your.proxy.server:port",
      "noProxy": "localhost,127.0.0.1,::1"
    }
  }
}

####


Build and then run the flask:

docker build -t flask-app:latest .

docker run --network=my_network -p 5000:5000 --name flask-app flask-app

Running with the env vars for mongo pw specified in env vars:

docker run --network=my_network -p 5000:5000   -e MONGO_USERNAME=admin -e MONGO_PASSWORD=*** -e MONGO_HOST=29471c688f24 -e MONGO_DB=reportsdb--name flask-app 

####

FROM ubuntu:latest  # Or ubuntu:20.04, ubuntu:22.04, etc.

# Install Python 3.9 and dependencies
RUN apt update && apt install -y software-properties-common && \
    add-apt-repository ppa:deadsnakes/ppa -y && \
    apt update && apt install -y python3.9 python3.9-venv python3.9-dev python3.9-distutils curl && \
    curl -sS https://bootstrap.pypa.io/get-pip.py | python3.9

# Set Python 3.9 as default
RUN update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.11 1 && \
    update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.9 2 && \
    update-alternatives --set python3 /usr/bin/python3.9

# Verify
RUN python3 --version && pip3 --version


####

@app.route('/health', methods=['GET'])
def health_check():
    return jsonify({"status": "ok"}), 200

####

import pytest
from app import app  # Adjust the import if your app file has a different name

@pytest.fixture
def client():
    app.config['TESTING'] = True
    with app.test_client() as client:
        yield client

def test_health_check(client):
    response = client.get('/health')
    assert response.status_code == 200
    assert response.get_json() == {"status": "ok"}

#####


pipeline {
    agent any

    environment {
        ARTIFACTORY_URL = 'https://your-artifactory.com'
        ARTIFACTORY_REPO_DEV = 'docker-local-dev'
        ARTIFACTORY_REPO_RELEASE = 'docker-local-release'
        IMAGE_NAME = 'your-app'
        ARTIFACTORY_USER = credentials('artifactory-user')
        ARTIFACTORY_PASSWORD = credentials('artifactory-password')

        ENVIRONMENT = sh(script: '''
            case "$JOB_NAME" in
                *dev*) echo "dev" ;;
                *stg*) echo "stg" ;;
                *prd*) echo "prd" ;;
                *) echo "unknown" ;;
            esac
        ''', returnStdout: true).trim()
    }

    stages {
        stage('Debug: Show Environment') {
            steps {
                script {
                    echo "üîç DEBUG: Detected Environment: ${env.ENVIRONMENT}"
                }
            }
        }

        stage('Build & Push Dev (Auto)') {
            when { expression { env.ENVIRONMENT == 'dev' } }
            steps {
                script {
                    def server = Artifactory.newServer url: ARTIFACTORY_URL
                    def rtDocker = Artifactory.docker server: server
                    def devTag = "${ARTIFACTORY_REPO_DEV}/${IMAGE_NAME}:latest-dev"

                    def buildInfo = rtDocker.push("${devTag}", forceXrayScan: true)
                    server.publishBuildInfo buildInfo
                    echo "‚úÖ Dev image pushed as latest-dev (overwritten)."
                }
            }
        }

        stage('Promote to Staging (Gated)') {
            when { expression { env.ENVIRONMENT == 'stg' } }
            steps {
                script {
                    def server = Artifactory.newServer url: ARTIFACTORY_URL
                    def promotionConfig = [
                        'targetRepo': ARTIFACTORY_REPO_RELEASE,
                        'dockerRepository': IMAGE_NAME,
                        'tag': 'latest-dev',
                        'targetTag': 'latest-stg',
                        'copy': true
                    ]

                    server.dockerPromote promotionConfig
                    echo "‚úÖ Staging image promoted from latest-dev to latest-stg."
                }
            }
        }

        stage('Verify Staging Deployment Before Prod') {
            when { expression { env.ENVIRONMENT == 'prd' } }
            steps {
                script {
                    echo "üîç Checking if latest-stg exists before proceeding to Production..."
                    def stgExists = sh(script: """
                        curl -s -u ${ARTIFACTORY_USER}:${ARTIFACTORY_PASSWORD} \\
                             -X GET "${ARTIFACTORY_URL}/artifactory/api/docker/${ARTIFACTORY_REPO_RELEASE}/v2/${IMAGE_NAME}/manifests/latest-stg" \\
                             -o /dev/null -w "%{http_code}"
                    """, returnStdout: true).trim()

                    if (stgExists != "200") {
                        error "‚ùå ERROR: Staging image does not exist in Artifactory. Cannot proceed to production."
                    }

                    echo "‚úÖ Staging image exists. Proceeding to Production."
                }
            }
        }

        stage('Promote to Production (Gated)') {
            when { expression { env.ENVIRONMENT == 'prd' } }
            steps {
                script {
                    def server = Artifactory.newServer url: ARTIFACTORY_URL
                    def promotionConfig = [
                        'targetRepo': ARTIFACTORY_REPO_RELEASE,
                        'dockerRepository': IMAGE_NAME,
                        'tag': 'latest-stg', // ‚úÖ Copy Staging, not Dev!
                        'targetTag': 'latest-prd',
                        'copy': true
                    ]

                    server.dockerPromote promotionConfig
                    echo "‚úÖ Production image copied from latest-stg to latest-prd."
                }
            }
        }
    }

    post {
        success {
            echo "üéâ Pipeline completed successfully for ${env.ENVIRONMENT}!"
        }
        failure {
            echo "‚ùå Pipeline failed for ${env.ENVIRONMENT}. Check logs."
        }
    }
}

####

Get Details of Blocking Queries

SELECT 
    r.session_id, 
    r.blocking_session_id, 
    r.wait_type, 
    r.wait_time / 1000 AS WaitTimeSec, 
    r.wait_resource, 
    t.text AS SQLQuery
FROM sys.dm_exec_requests r
CROSS APPLY sys.dm_exec_sql_text(r.sql_handle) t
WHERE r.blocking_session_id <> 0;

####

Find Queries Currently Running in the Usage Database

USE WSS_UsageApplication; -- Change to your SharePoint DB
GO

SELECT 
    r.session_id, 
    r.status, 
    r.command, 
    r.wait_type, 
    r.wait_time / 1000 AS WaitTimeSec, 
    r.blocking_session_id, 
    t.text AS SQLQuery
FROM sys.dm_exec_requests r
CROSS APPLY sys.dm_exec_sql_text(r.sql_handle) t
WHERE r.database_id = DB_ID();

####

Find the Most Blocked Queries (Historical Data)

SELECT 
    blocking_session_id AS BlockingSession, 
    COUNT(*) AS BlockedQueries
FROM sys.dm_exec_requests
WHERE blocking_session_id <> 0
GROUP BY blocking_session_id
ORDER BY COUNT(*) DESC;

####

Find the Largest Tables in the Usage DB (Possible Cause of Blocking)

USE WSS_UsageApplication; -- Change this to your Usage DB
GO

SELECT 
    t.NAME AS TableName, 
    p.rows AS RowCounts, 
    SUM(a.total_pages) * 8 / 1024 AS TotalSpaceMB
FROM sys.tables t
JOIN sys.indexes i ON t.OBJECT_ID = i.object_id
JOIN sys.partitions p ON i.object_id = p.OBJECT_ID AND i.index_id = p.index_id
JOIN sys.allocation_units a ON p.partition_id = a.container_id
GROUP BY t.Name, p.Rows
ORDER BY TotalSpaceMB DESC;

import requests

def getOAuthAccessToken(host, endpoint, basicAuthString):
    url = f"https://{host}{endpoint}"  # Construct full URL
    payload = {'grant_type': 'client_credentials'}  # Use a dictionary for form data
    headers = {
        'Content-Type': 'application/x-www-form-urlencoded',
        'Authorization': basicAuthString
    }

    # Make the POST request
    response = requests.post(url, data=payload, headers=headers)

    # Raise an error if the request fails
    response.raise_for_status()

    # Parse JSON response
    json_data = response.json()

    # Return access token
    return json_data.get("access_token", None)

def makeBearerCall(host, token, verb, endpoint):
    url = f"https://{host}{endpoint}"  # Construct full URL
    headers = {
        'Authorization': f'Bearer {token}'
    }

    # Make the request with the specified HTTP verb
    response = requests.request(verb, url, headers=headers)

    # Raise an error if the request fails
    response.raise_for_status()

    # Return response content as text
    return response.text

    import requests
import json

# Define the URL of the web service
url = "https://example.com/api/data"  # Replace with the actual API endpoint

try:
    # Make a GET request to the web service
    response = requests.get(url)

    # Ensure the request was successful
    response.raise_for_status()

    # Parse the JSON response
    data = response.json()

    # Print the parsed JSON data in a structured format
    for entry in data:
        print(f"SUB ID: {entry.get('SUB ID', 'N/A')}")
        print(f"SERVER: {entry.get('SERVER', 'N/A')}")
        print(f"ECOSYSTEM: {entry.get('ECOSYSTEM', 'N/A')}")
        print("-" * 30)

except requests.exceptions.RequestException as e:
    print(f"Error fetching data: {e}")
except json.JSONDecodeError:
    print("Error parsing JSON response")

import requests

url = "https://your-api-endpoint.com"  # Replace with actual API URL
try:
    response = requests.get(url)
    response.raise_for_status()
except requests.exceptions.SSLError as e:
    print(f"SSL Error: {e}")
except requests.exceptions.RequestException as e:
    print(f"Request failed: {e}")

import requests
from requests.adapters import HTTPAdapter
from urllib3.util.ssl_ import create_urllib3_context

class TLSAdapter(HTTPAdapter):
    def init_poolmanager(self, *args, **kwargs):
        ctx = create_urllib3_context()
        ctx.minimum_version = 3  # Forces TLS 1.2 or higher
        kwargs['ssl_context'] = ctx
        super().init_poolmanager(*args, **kwargs)

session = requests.Session()
session.mount("https://", TLSAdapter())

url = "https://your-api-endpoint.com"
response = session.get(url, verify=False)  # Set verify=False only for debugging
print(response.text)

[System.Net.WebRequest]::DefaultWebProxy = New-Object System.Net.WebProxy("http://proxyserver:8080", $true)
[System.Net.WebRequest]::DefaultWebProxy.Credentials = [System.Net.CredentialCache]::DefaultCredentials


sudo nano /etc/systemd/system/mycontainer.service

[Unit]
Description=My Docker Container
After=network.target

[Service]
ExecStartPre=-/usr/bin/docker pull your-image:latest
ExecStart=/usr/bin/docker run --rm --name mycontainer -p 8080:8080 -d your-image:latest
ExecStop=/usr/bin/docker stop mycontainer
Restart=always

[Install]
WantedBy=multi-user.target

sudo systemctl daemon-reload
sudo systemctl enable mycontainer
sudo systemctl start mycontainer

######

from flask import Flask, request, jsonify, render_template, send_file, redirect, url_for
from flask_pymongo import PyMongo
from flask_oidc import OpenIDConnect
from bson.objectid import ObjectId
from datetime import datetime
import os
import pandas as pd
from io import BytesIO

app = Flask(__name__)

# --- OIDC Config ---
app.config.update({
    'SECRET_KEY': 'super-secret-key',  # Replace with your own secret
    'OIDC_CLIENT_SECRETS': 'client_secrets.json',
    'OIDC_SCOPES': ['openid', 'email', 'profile'],
    'OIDC_RESOURCE_CHECK_AUD': True,
    'OIDC_INTROSPECTION_AUTH_METHOD': 'client_secret_post',
    'OVERWRITE_REDIRECT_URI': 'http://localhost:5000/oidc/callback'  # Update in production
})
oidc = OpenIDConnect(app)

# --- MongoDB Config ---
app.config["MONGO_URI"] = "mongodb://admin:@localhost:27017/reportsdb?authSource=admin"
mongo = PyMongo(app)
reports_collection = mongo.db.reports

# --- Auth routes ---
@app.route('/login')
@oidc.require_login
def login():
    return redirect(url_for('view_reports'))

@app.route('/logout')
def logout():
    oidc.logout()
    return redirect(url_for('view_reports'))

# --- UI route (protected) ---
@app.route('/reports/view', methods=['GET'])
@oidc.require_login
def view_reports():
    # (existing logic for view_reports goes here)
    return render_template('view_reports.html')

# --- OIDC callback ---
@app.route('/oidc/callback')
def oidc_callback():
    return redirect(url_for('view_reports'))

if __name__ == '__main__':
    app.run(debug=True)

#####

{
  "web": {
    "client_id": "flask-ui-client",
    "client_secret": "YOUR_CLIENT_SECRET",
    "auth_uri": "https://your-oidc-provider/auth",
    "token_uri": "https://your-oidc-provider/token",
    "userinfo_uri": "https://your-oidc-provider/userinfo",
    "issuer": "https://your-oidc-provider/",
    "redirect_uris": [
      "http://localhost:5000/oidc/callback"
    ]
  }
}

<link rel="icon" type="image/png" href="{{ url_for('static', filename='mf-favicon.png') }}">

##

# Load SharePoint snapin if needed
if ((Get-PSSnapin -Name Microsoft.SharePoint.PowerShell -ErrorAction SilentlyContinue) -eq $null) {
    Add-PSSnapin Microsoft.SharePoint.PowerShell
}

# Variables
$siteUrl = "http://your-sharepoint-site"
$libraryName = "Documents"

# Connect
$site = Get-SPSite $siteUrl
$web = $site.RootWeb
$list = $web.Lists[$libraryName]

# Function to count folders recursively
function Get-FolderCount {
    param (
        [Microsoft.SharePoint.SPFolder]$Folder
    )

    $folderCount = 0

    foreach ($subFolder in $Folder.SubFolders) {
        if ($subFolder.Name -ne "Forms") { # Skip system folders
            $folderCount++
            $folderCount += Get-FolderCount -Folder $subFolder
        }
    }

    return $folderCount
}

# Start counting from root folder
$totalFolders = Get-FolderCount -Folder $list.RootFolder

Write-Host "Total number of folders in library '$libraryName': $totalFolders"

# Cleanup
$web.Dispose()
$site.Dispose()

####

SELECT 
    DocId,
    COUNT(*) AS VersionCount,
    MAX(Version) AS MaxVersion,
    LeafName,
    DirName
FROM 
    AllDocVersions WITH (NOLOCK)
WHERE 
    LeafName LIKE '%.one%' AND
    DirName LIKE 'sites/marketing%'  -- Adjust to your site's path
GROUP BY 
    DocId, LeafName, DirName
ORDER BY 
    VersionCount DESC;

###

SELECT COUNT(*) AS VersionRowCount
FROM AllDocVersions WITH (NOLOCK)
WHERE SiteId = 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx';

Add-PSSnapin Microsoft.SharePoint.PowerShell -ErrorAction SilentlyContinue

$results = @()

$sites = Get-SPSite -Limit All

foreach ($site in $sites) {
    try {
        # Check if the Publishing feature is activated at the Site Collection level
        $siteFeatures = Get-SPFeature -Site $site -ErrorAction SilentlyContinue
        if ($siteFeatures.DisplayName -contains "PublishingSite") {
            $results += [PSCustomObject]@{
                Url      = $site.Url
                Level    = "Site Collection"
                Feature  = "PublishingSite"
            }
        }

        foreach ($web in $site.AllWebs) {
            try {
                $webFeatures = Get-SPFeature -Web $web -ErrorAction SilentlyContinue
                if ($webFeatures.DisplayName -contains "PublishingWeb") {
                    $results += [PSCustomObject]@{
                        Url      = $web.Url
                        Level    = "Web"
                        Feature  = "PublishingWeb"
                    }
                }
            } catch {
                Write-Warning "Error accessing web: $($_.Exception.Message)"
            } finally {
                $web.Dispose()
            }
        }

    } catch {
        Write-Warning "Error accessing site collection: $($_.Exception.Message)"
    } finally {
        $site.Dispose()
    }
}

# Output results
$results | Format-Table -AutoSize

# Optional: Export to CSV
$results | Export-Csv "C:\Temp\PublishingSites.csv" -NoTypeInformation

Write-Host "‚úÖ Finished scanning for publishing-enabled sites." -ForegroundColor Green


###
Add-PSSnapin Microsoft.SharePoint.PowerShell -ErrorAction SilentlyContinue

# Build the override dictionary and category name list
$overrideDictionary = @{}
$keyList = @()

$svc = [Microsoft.SharePoint.Administration.SPDiagnosticsService]::Local

foreach ($area in $svc.Areas) {
    foreach ($category in $area.Categories) {
        $fullName = "$($area.Name)\$($category.Name)"
        if (-not $overrideDictionary.ContainsKey($fullName)) {
            $overrideDictionary[$fullName] = $category.TraceSeverity
            $keyList += $fullName
        }
    }
}

Write-Host "`nTotal diagnostic categories loaded: $($keyList.Count)"
Write-Host "Simulating dictionary lookups..."

# Simulate dictionary lookup (like TraceLevelOverrideStop does internally)
$sw = [System.Diagnostics.Stopwatch]::StartNew()

foreach ($key in $keyList) {
    $null = $overrideDictionary[$key]
}

$sw.Stop()

Write-Host "`n‚úÖ Simulation complete"
Write-Host "‚è± Lookup time: $($sw.ElapsedMilliseconds) ms"
Write-Host "üîç Keys checked: $($keyList.Count)"
Write-Host "üì¶ Dictionary size: $($overrideDictionary.Count)"

####

$targetTime = Get-Date "2025-06-20 14:10"
$before = $targetTime.AddMinutes(5)
$after = $targetTime.AddMinutes(-5)

Get-SPTimerJob | Where-Object {
    $_.LastRunTime -ge $after -and $_.LastRunTime -le $before
} | Select DisplayName, LastRunTime, TypeName

###

Title:
High CPU usage in w3wp.exe and owstimer.exe following SharePoint 2016 patch (16.0.5478.1000), linked to ULS.SendTrace and Dictionary.TryGetValue

üß© Problem Overview:
Since deploying the March 2024 CU (build 16.0.5478.1000) to our SharePoint 2016 farms, we have observed intermittent but severe CPU spikes, impacting both:

w3wp.exe worker processes (App Pools)

owstimer.exe (SharePoint Timer Service)

The behavior is consistent across multiple farms, in different geographic regions, with different AV policies and different usage patterns ‚Äî but all running Server 2019 and patched to this build.

üîç Observed Technical Behavior:
1. w3wp.exe High CPU (App Pools)
Spikes are traced in Dynatrace to:

ULS.SendTrace

TraceRaiseEventMgdHandler

.NET Dictionary<TKey,TValue>.TryGetValue

CPU usage occurs even with:

Default logging levels

No custom trace categories

Publishing-related logging disabled

Affected categories include SharePoint.Publishing, but also others, suggesting a global trace evaluation issue, not just one area.

2. owstimer.exe CPU Spikes
owstimer.exe intermittently shows similar high CPU usage

Not instrumented in Dynatrace ‚Äî but we observed logging activity to:

C:\Users\...\AppData\Local\Temp\DCacheAdministration...

Suggests Distributed Cache or configuration-related timer jobs are triggering trace logic internally

ULS logs show the same Dictionary.TryGetValue activity in ULS.SendTrace

‚ùå What We've Ruled Out
Potential Cause	Status
AV interference	‚ùå Ruled out ‚Äî proper exclusions confirmed across all farms
Excessive trace volume	‚ùå Logging set to None for all categories ‚Äî issue persists
Misconfigured logging overrides	‚ùå Clear-SPLogLevel run; all log levels confirmed at default
Custom solutions	‚úÖ We have reviewed deployed WSPs and nothing changed recently
Publishing alone	‚ùå Disabling Publishing logging had no effect
Timer job misfire	üü° Inconclusive, but no unusual jobs found in logs during spikes

üß™ Conclusion / Hypothesis
There appears to be a regression in trace logging logic introduced by the March 2024 CU, whereby:

Logging category dictionary evaluations are invoked on every request, or background job

These dictionary lookups are not efficiently cached, or are being repeatedly re-evaluated

It impacts trace path evaluation, even when logging output is fully disabled

The behavior did not occur prior to this patch and is only present on Server 2019 hosts

üìù What We Need from Microsoft
Confirmation of changes to trace evaluation, logging override logic, or diagnostic infrastructure in/around 16.0.5478.1000

Review of known issues involving:

ULS.SendTrace

SPDiagnosticsServiceBase

Publishing modules and trace categories

Suggested hotfix or rollback plan (if applicable)

A reproducible scenario (or test guidance) we can use to confirm this without impacting live usage

üí¨ Optional Additional Notes
Some affected endpoints during spikes: sites.asmx, sharedaccess.asmx, GetUpdatedFormDigest

We attempted to trigger the issue via synthetic traffic to publishing sites ‚Äî no immediate effect

The farms are otherwise stable, but trace-related CPU overhead is degrading performance significantly

###

# Load SharePoint snap-in if not already loaded
if ((Get-PSSnapin -Name Microsoft.SharePoint.PowerShell -ErrorAction SilentlyContinue) -eq $null) {
    Add-PSSnapin Microsoft.SharePoint.PowerShell
}

# Get all logging categories
$allCategories = Get-SPLogLevel

# Loop through and set the levels
foreach ($category in $allCategories) {
    Set-SPLogLevel -TraceSeverity High -EventSeverity Error -Area $category.Area -Name $category.Name
}

Write-Host "All logging categories updated: TraceSeverity = High, EventSeverity = Error"

###

$logRoot = "C:\ProgramData\dynatrace\oneagent\log"
$since = (Get-Date).AddDays(-1)
$keywords = @("LogRequest", "timeout", "PurePath", "span", "callback", "Async", "Exception", "flush", "export", "took", "duration")

# Get all log files modified in the last day
$logFiles = Get-ChildItem -Path $logRoot -Recurse -Include *.log -File |
    Where-Object { $_.LastWriteTime -gt $since }

foreach ($file in $logFiles) {
    try {
        $lines = Get-Content $file.FullName
        $matches = $lines | Where-Object {
            $line = $_.ToLower()
            $keywords | ForEach-Object { $line -like "*$_*" }
        }
        if ($matches) {
            Write-Host "`n===== Matches in $($file.FullName) =====" -ForegroundColor Cyan
            $matches | ForEach-Object { Write-Host $_ -ForegroundColor Yellow }
        }
    }
    catch {
        Write-Warning "Failed to read $($file.FullName): $_"
    }
}

"applicationMatchRules": [
  {
    "applicationMatchType": "MATCHES",
    "matchers": [
      {
        "matcherType": "DOMAIN",
        "value": "app1.example.com"
      },
      {
        "matcherType": "URL_STARTS_WITH",
        "value": "https://example.com/path"
      }
    ]
  }
]

#####

# CONFIG
$logFile = "C:\Logs\AD_HealthMonitor.log"
$intervalSeconds = 60
$domainName = (Get-WmiObject Win32_ComputerSystem).Domain

# Ensure log folder exists
$logFolder = [System.IO.Path]::GetDirectoryName($logFile)
if (!(Test-Path $logFolder)) {
    New-Item -ItemType Directory -Path $logFolder | Out-Null
}

Write-Host "Starting AD health monitor loop. Logging to $logFile"

while ($true) {
    $timestamp = Get-Date -Format "yyyy-MM-dd HH:mm:ss"
    $dc = "UNKNOWN"
    $ldapTime = "N/A"
    $nltestDc = "UNKNOWN"
    $nltestError = $null
    $siteName = (nltest /dsgetsite 2>&1)[0].Trim()

    try {
        # Get domain controller via .NET
        $domain = [System.DirectoryServices.ActiveDirectory.Domain]::GetCurrentDomain()
        $dc = $domain.PdcRoleOwner.Name

        # LDAP query timing
        $sw = [System.Diagnostics.Stopwatch]::StartNew()
        $searcher = New-Object System.DirectoryServices.DirectorySearcher
        $searcher.Filter = "(objectClass=user)"
        $searcher.PageSize = 1
        $result = $searcher.FindOne()
        $sw.Stop()
        $ldapTime = $sw.ElapsedMilliseconds
        $status = if ($result) { "OK" } else { "NoResult" }

    } catch {
        $status = "LDAP_FAIL: $($_.Exception.Message)"
    }

    try {
        # Get DC info using nltest
        $nltestOutput = nltest /dsgetdc:$domainName 2>&1
        if ($LASTEXITCODE -eq 0) {
            $nltestDc = ($nltestOutput | Select-String "DC:" | ForEach-Object { ($_ -split ":")[1].Trim() })[0]
        } else {
            $nltestError = $nltestOutput -join " "
        }
    } catch {
        $nltestError = $_.Exception.Message
    }

    $logLine = "$timestamp | AD Site: $siteName | PDC: $dc | LDAP: ${ldapTime}ms | Status: $status | NLTEST_DC: $nltestDc"

    if ($nltestError) {
        $logLine += " | NLTEST_ERR: $nltestError"
    }

    Add-Content -Path $logFile -Value $logLine
    Write-Host $logLine

    Start-Sleep -Seconds $intervalSeconds
}

###

# CONFIG
$logFile = "C:\Logs\AD_HealthMonitor_EnsureUser.log"
$intervalSeconds = 60
$domainName = (Get-WmiObject Win32_ComputerSystem).Domain
$siteName = (nltest /dsgetsite 2>&1)[0].Trim()
$webUrl = "https://your.site.url"
$usersToCheck = @(
    "i:0#.w|DOMAIN1\user1",
    "i:0#.w|DOMAIN2\user2"
)

# Ensure log folder exists
$logFolder = [System.IO.Path]::GetDirectoryName($logFile)
if (!(Test-Path $logFolder)) {
    New-Item -ItemType Directory -Path $logFolder | Out-Null
}

Write-Host "Starting AD + SharePoint EnsureUser monitor loop..."

# Get SPWeb context once
try {
    $web = Get-SPWeb $webUrl
} catch {
    Write-Error "Failed to open SPWeb at $webUrl. Exiting."
    return
}

while ($true) {
    $timestamp = Get-Date -Format "yyyy-MM-dd HH:mm:ss"
    $dc = "UNKNOWN"
    $ldapTime = "N/A"
    $nltestDc = "UNKNOWN"
    $nltestError = $null
    $logLines = @()

    try {
        # Get DC via .NET
        $domain = [System.DirectoryServices.ActiveDirectory.Domain]::GetCurrentDomain()
        $dc = $domain.PdcRoleOwner.Name

        # Time a basic LDAP user query
        $sw = [System.Diagnostics.Stopwatch]::StartNew()
        $searcher = New-Object System.DirectoryServices.DirectorySearcher
        $searcher.Filter = "(objectClass=user)"
        $searcher.PageSize = 1
        $result = $searcher.FindOne()
        $sw.Stop()
        $ldapTime = $sw.ElapsedMilliseconds
        $status = if ($result) { "OK" } else { "NoResult" }
    } catch {
        $status = "LDAP_FAIL: $($_.Exception.Message)"
    }

    try {
        # DC detection via nltest
        $nltestOutput = nltest /dsgetdc:$domainName 2>&1
        if ($LASTEXITCODE -eq 0) {
            $nltestDc = ($nltestOutput | Select-String "DC:" | ForEach-Object { ($_ -split ":")[1].Trim() })[0]
        } else {
            $nltestError = $nltestOutput -join " "
        }
    } catch {
        $nltestError = $_.Exception.Message
    }

    $logLines += "$timestamp | AD Site: $siteName | PDC: $dc | LDAP: ${ldapTime}ms | Status: $status | NLTEST_DC: $nltestDc"
    if ($nltestError) {
        $logLines += "$timestamp | NLTEST_ERR: $nltestError"
    }

    # Now run EnsureUser for each test user
    foreach ($loginName in $usersToCheck) {
        try {
            $eusw = [System.Diagnostics.Stopwatch]::StartNew()
            $spUser = $web.EnsureUser($loginName)
            $eusw.Stop()
            $eusElapsed = $eusw.ElapsedMilliseconds
            $eusStatus = "OK"
        } catch {
            $eusw.Stop()
            $eusElapsed = $eusw.ElapsedMilliseconds
            $eusStatus = "FAIL: $($_.Exception.Message)"
        }
        $logLines += "$timestamp | EnsureUser | $loginName | Time: ${eusElapsed}ms | Status: $eusStatus"
    }

    # Output and write to file
    $logLines | ForEach-Object {
        Write-Host $_
        Add-Content -Path $logFile -Value $_
    }

    Start-Sleep -Seconds $intervalSeconds
}

####

[
  {
    "UserProfile": {
      "EmployeeId": "THEIR_EMP_ID",
      "SamAccountName": "DOMAIN\\them",
      "EmailId": "them@example.com"
    },
    "SiteCollection": {
      "SPGuid": "actual-guid-from-site"
    }
  }
]

####

$bucket = "your-bucket-name"
$prefix = "out/"
$localDownloadPath = "C:\path\to\download"
$cutoffTime = (Get-Date).ToUniversalTime().AddDays(-1)

# Ensure download path exists
if (-not (Test-Path $localDownloadPath)) {
    New-Item -ItemType Directory -Path $localDownloadPath | Out-Null
}

# Get all files in the "out/" folder
$objects = aws s3api list-objects-v2 --bucket $bucket --prefix $prefix | ConvertFrom-Json

$matchedObjects = $objects.Contents |
    Where-Object {
        $_.Key -like "*reports*" -and
        ([datetime]$_.LastModified) -gt $cutoffTime
    }

foreach ($obj in $matchedObjects) {
    $s3Key = $obj.Key
    $relativePath = $s3Key.Substring($prefix.Length)  # Remove 'out/' from the start
    $localFilePath = Join-Path $localDownloadPath $relativePath
    $localDir = Split-Path $localFilePath

    if (-not (Test-Path $localDir)) {
        New-Item -ItemType Directory -Path $localDir -Force | Out-Null
    }

    Write-Host "Downloading $s3Key ‚Üí $localFilePath"
    aws s3 cp "s3://$bucket/$s3Key" "$localFilePath"
}

- name: Check if .NET SDK 8.0.412 is installed
  win_reg_stat:
    path: HKLM:\SOFTWARE\Microsoft\Windows\CurrentVersion\Uninstall\{A9535A07-8759-46F3-A1A7-1A773E7289F7}
  register: dotnet_installed

- name: Install .NET SDK
  win_command: '"{{ temp_area }}\\{{ dotnet_msi }}" /install /quiet /norestart'
  when: not dotnet_installed.exists

###

[ req ]
default_bits       = 2048
distinguished_name = req_distinguished_name
prompt             = yes

[ req_distinguished_name ]
C  = Country Name (2 letter code)
C_default = FR

ST = State or Province Name (full name)
ST_default = Paris

O  = Organization Name (eg, company)
O_default = company

CN = Common Name (e.g., server FQDN or YOUR name)
CN_default = myapp-client-cert

###

openssl_conf = default_conf
[ default_conf ]
ssl_conf = ssl_sect
[ ssl_sect ]
system_default = system_default_sect
[ system_default_sect ]
CipherString = DEFAULT:@SECLEVEL=1

###

TESTING

$env:OPENSSL_CONF="C:\path\to\openssl_client.cnf"
python your_script.py

###

PERM 

setx OPENSSL_CONF "C:\path\to\openssl_client.cnf"

###

import ssl

# Monkey-patch the default SSL context to allow legacy ciphers + force TLS1.2
_orig_create_default_context = ssl.create_default_context

def _legacy_context(*args, **kwargs):
    ctx = _orig_create_default_context(*args, **kwargs)
    ctx.set_ciphers("DEFAULT:@SECLEVEL=1")  # drop seclevel
    ctx.minimum_version = ssl.TLSVersion.TLSv1_2
    ctx.maximum_version = ssl.TLSVersion.TLSv1_2
    return ctx

ssl.create_default_context = _legacy_context

#####

_old = sslmod.create_urllib3_context
def _patched(*a, **k):
    c = _old(*a, **k)
    # Allow all legacy suites OpenSSL 3 will permit at seclevel 1
    c.set_ciphers("ALL:@SECLEVEL=1")
    # Force TLS 1.2 (server has no TLS1.3)
    c.minimum_version = ssl.TLSVersion.TLSv1_2
    c.maximum_version = ssl.TLSVersion.TLSv1_2
    # Help with some buggy servers
    if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
        c.options |= ssl.OP_LEGACY_SERVER_CONNECT
    return c

sslmod.create_urllib3_context = _patched

#####

#!/usr/bin/env python3
import os, ssl, traceback
import requests
from requests.adapters import HTTPAdapter
from urllib3.util import create_urllib3_context

# ====== CONFIG ======
URL = "#"   # change path if needed
HEADERS = {"Authorization": "Basic <redacted>"}     # set if needed

# If you have a client cert (mTLS), set one of these:
CLIENT_CERT_PEM = None  # e.g. r"C:\path\client.crt"
CLIENT_KEY_PEM  = None  # e.g. r"C:\path\client.key"

PFX_FILE = None         # e.g. r"C:\path\client.pfx"
PFX_PASS = ""           # password if needed
# =====================

def attempt(name, session_builder):
    print(f"\n=== {name} ===")
    try:
        s = session_builder()
        r = s.post(URL, headers=HEADERS, timeout=20)
        print("STATUS:", r.status_code)
        print("OK, got response bytes:", len(r.content))
    except Exception as e:
        print("ERROR:", repr(e))
        traceback.print_exc(limit=1)

def vanilla_session():
    s = requests.Session()
    return s

def legacy_ctx():
    ctx = create_urllib3_context()
    ctx.set_ciphers("DEFAULT:@SECLEVEL=1")
    ctx.minimum_version = ssl.TLSVersion.TLSv1_2
    ctx.maximum_version = ssl.TLSVersion.TLSv1_2
    return ctx

def legacy_session():
    s = requests.Session()
    s.mount("https://", HTTPAdapter(ssl_context=legacy_ctx()))
    return s

def legacy_no_proxy_session():
    s = legacy_session()
    # ensure proxies won't intercept this host
    host = requests.utils.urlparse(URL).hostname
    os.environ["NO_PROXY"] = host
    os.environ["no_proxy"] = host
    s.trust_env = True
    return s

def legacy_with_client_cert_session():
    s = legacy_session()
    if CLIENT_CERT_PEM and CLIENT_KEY_PEM:
        s.cert = (CLIENT_CERT_PEM, CLIENT_KEY_PEM)
        return s
    if PFX_FILE:
        # requires: pip install requests-pkcs12
        from requests_pkcs12 import Pkcs12Adapter
        s.mount("https://", Pkcs12Adapter(pkcs12_filename=PFX_FILE,
                                          pkcs12_password=PFX_PASS))
        return s
    raise RuntimeError("No client cert configured (PEM or PFX).")

if __name__ == "__main__":
    print("Python:", ssl.OPENSSL_VERSION)
    print("Proxies from env:",
          {k: v for k, v in os.environ.items() if k.lower() in ("http_proxy","https_proxy","no_proxy","no_proxy")})

    attempt("1) vanilla", vanilla_session)
    attempt("2) legacy seclevel=1 (TLS1.2)", legacy_session)
    attempt("3) legacy + NO_PROXY for host", legacy_no_proxy_session)

    if any([CLIENT_CERT_PEM and CLIENT_KEY_PEM, PFX_FILE]):
        attempt("4) legacy + client cert", legacy_with_client_cert_session)
    else:
        print("\n(Skipping mTLS test ‚Äì no client cert configured)")

###

#!/usr/bin/env python3
import ssl
import requests
import urllib3.util.ssl_ as sslmod

# ---- TLS monkey-patch ----
_old = sslmod.create_urllib3_context
def _patched(*a, **k):
    ctx = _old(*a, **k)
    ctx.set_ciphers("ALL:@SECLEVEL=1")
    ctx.minimum_version = ssl.TLSVersion.TLSv1_2
    ctx.maximum_version = ssl.TLSVersion.TLSv1_2
    if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
        ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
    return ctx
sslmod.create_urllib3_context = _patched
# --------------------------

URL = "#"

try:
    r = requests.get(URL, timeout=20)
    print("STATUS:", r.status_code)
    print("BODY:", r.text[:200], "...")
except Exception as e:
    print("ERROR:", repr(e))

####

# tls_handshake_probe.py
import ssl, socket

HOST = ""
PORT = 443

CIPHERS = [
    "AES128-GCM-SHA256",
    "AES256-GCM-SHA384",
    "AES128-SHA256",
    "AES256-SHA256",
    "AES128-SHA",
    "AES256-SHA",
    "DES-CBC3-SHA",
]

def try_once(cipher, server_hostname=True):
    ctx = ssl.SSLContext(ssl.PROTOCOL_TLSv1_2)
    ctx.set_ciphers(f"{cipher}:@SECLEVEL=1")
    if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
        ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
    try:
        with socket.create_connection((HOST, PORT), timeout=8) as sock:
            hn = HOST if server_hostname else None  # toggle SNI
            with ctx.wrap_socket(sock, server_hostname=hn) as ss:
                print(f"OK  {cipher:20s} ‚Üí {ss.version()}  {ss.cipher()}")
                return True
    except Exception as e:
        print(f"FAIL {cipher:20s} ‚Üí {e!r}")
        return False

print("=== With SNI ===")
any_ok = False
for c in CIPHERS:
    any_ok |= try_once(c, server_hostname=True)

print("\n=== Without SNI ===")
for c in CIPHERS:
    any_ok |= try_once(c, server_hostname=False)

print("\nRESULT:", "at least one handshake succeeded" if any_ok else "all handshakes failed")

####

import ssl
import urllib3.util.ssl_ as sslmod

_old = sslmod.create_urllib3_context
def _patched(*a, **k):
    ctx = _old(*a, **k)
    ctx.set_ciphers("AES128-GCM-SHA256:@SECLEVEL=1")
    ctx.minimum_version = ssl.TLSVersion.TLSv1_2
    ctx.maximum_version = ssl.TLSVersion.TLSv1_2
    return ctx

sslmod.create_urllib3_context = _patched


###

import ssl
import urllib3.util.ssl_ as sslmod

_old = sslmod.create_urllib3_context
def _patched(*a, **k):
    ctx = _old(*a, **k)
    ctx.set_ciphers("AES128-GCM-SHA256:@SECLEVEL=1")
    ctx.minimum_version = ssl.TLSVersion.TLSv1_2
    ctx.maximum_version = ssl.TLSVersion.TLSv1_2
    return ctx

sslmod.create_urllib3_context = _patched

###

#!/usr/bin/env python3
"""
simpleCheck.py
- patch urllib3's SSL context to force a compatible TLS1.2 cipher (AES128-GCM-SHA256)
- ensure requests bypasses proxies for the target host
- perform a simple GET and print result
"""

import os
# ensure we bypass proxies for the target host (lower/upper to be safe)
TARGET_HOST = ""
os.environ["NO_PROXY"] = TARGET_HOST
os.environ["no_proxy"] = TARGET_HOST

import ssl
import requests

# monkey-patch urllib3's context factory so requests/urllib3 uses our cipher/seclevel
try:
    import urllib3.util.ssl_ as sslmod
except Exception:
    # older installs might be slightly different; fall back to urllib3.util.ssl_
    import urllib3.util.ssl_ as sslmod

_old = sslmod.create_urllib3_context

def _patched(*a, **k):
    ctx = _old(*a, **k)
    # force a cipher we proved works and allow legacy security level 1
    ctx.set_ciphers("AES128-GCM-SHA256:@SECLEVEL=1")
    # restrict to TLS1.2 (server doesn't offer TLS1.3)
    ctx.minimum_version = ssl.TLSVersion.TLSv1_2
    ctx.maximum_version = ssl.TLSVersion.TLSv1_2
    # help with some buggy servers if available
    if hasattr(ssl, "OP_LEGACY_SERVER_CONNECT"):
        ctx.options |= ssl.OP_LEGACY_SERVER_CONNECT
    return ctx

sslmod.create_urllib3_context = _patched

# Build session and force it to ignore environment proxies (use direct connection)
s = requests.Session()
s.trust_env = False   # ignore HTTP(S)_PROXY env vars, use direct connection

URL = f"https://{TARGET_HOST}/api/v2/tokens"  # change path as needed

try:
    r = s.get(URL, timeout=20)
    print("STATUS:", r.status_code)
    print("BODY (first 400 chars):")
    print(r.text[:400])
except Exception as e:
    print("ERROR:", repr(e))
    # print a little more detail for debugging
    import traceback
    traceback.print_exc(limit=1)

###

#!/usr/bin/env python3
# --- put this BEFORE importing requests ---
import os, ssl
import urllib3.util.ssl_ as sslmod

TARGET_HOST = ""

# bypass proxies for this host and ignore env proxies entirely
os.environ["NO_PROXY"] = TARGET_HOST
os.environ["no_proxy"] = TARGET_HOST

_old = sslmod.create_urllib3_context
def _patched(*a, **k):
    ctx = _old(*a, **k)
    ctx.set_ciphers("AES128-GCM-SHA256:@SECLEVEL=1")
    ctx.minimum_version = ssl.TLSVersion.TLSv1_2
    ctx.maximum_version = ssl.TLSVersion.TLSv1_2
    return ctx
sslmod.create_urllib3_context = _patched
# -----------------------------------------

import requests

s = requests.Session()
s.trust_env = False  # ignore HTTP(S)_PROXY completely

URL = f"https://{TARGET_HOST}/api/v2/tokens"  # adjust path if needed
try:
    r = s.get(URL, timeout=20)
    print("STATUS:", r.status_code)
    print("BODY:", r.text[:300], "...")
except Exception as e:
    print("ERROR:", repr(e))

# Build path to user's local AppData
$pyIniPath = Join-Path $env:LOCALAPPDATA "py.ini"

# Content we want
$iniContent = @"
[defaults]
python=3.10
"@

# Write it out (overwrite if exists)
Set-Content -Path $pyIniPath -Value $iniContent -Encoding ASCII

Write-Host "Created py.ini at $pyIniPath pointing default to Python 3.10"


###

Check if openssl header there:

dpkg -l | grep libssl-dev

Check openssl

apt-get install -y libssl-dev zlib1g-dev libpcre3 libpcre3-dev build-essential pkg-config

Rebuild from source

rm -rf /tmp/nginx-1.26.3
tar -xzf /tmp/nginx-1.26.3.tar.gz -C /tmp
cd /tmp/nginx-1.26.3
./configure --with-http_ssl_module --with-pcre-jit \
  --prefix=/usr/share/nginx --conf-path=/etc/nginx/nginx.conf \
  --pid-path=/run/nginx.pid --modules-path=/usr/lib/nginx/modules \
  --http-log-path=/var/log/nginx/access.log --error-log-path=/var/log/nginx/error.log
make -j"$(nproc)"
make install

####

- name: Configure nginx (with SSL)
  command:
    argv:
      - ./configure
      - --with-http_ssl_module
      - --with-pcre-jit
      - --prefix=/usr/share/nginx
      - --conf-path=/etc/nginx/nginx.conf
      - --pid-path=/run/nginx.pid
      - --modules-path=/usr/lib/nginx/modules
      - --http-log-path=/var/log/nginx/access.log
      - --error-log-path=/var/log/nginx/error.log
      - --with-compat
      - --with-debug
      - --with-http_stub_status_module
      - --with-http_realip_module
      - --with-http_auth_request_module
      - --with-http_v2_module
      - --with-http_dav_module
      - --with-http_slice_module
      - --with-threads
      - --with-http_addition_module
      - --with-http_gunzip_module
      - --with-http_gzip_static_module
      - --with-http_sub_module
  args:
    chdir: "/tmp/{{ nginx_name }}"

    ###

    ./configure --prefix=/usr/share/nginx --conf-path=/etc/nginx/nginx.conf --lock-path=/var/lock/nginx.lock --pid-path=/run/nginx.pid --modules-path=/usr/lib/nginx/modules --http-log-path=/var/log/nginx/access.log --error-log-path=/var/log/nginx/error.log --with-compat --with-debug --with-pcre-jit --with-http_ssl_module --with-http_stub_status_module --with-http_realip_module --with-http_auth_request_module --with-http_v2_module --with-http_dav_module --with-http_slice_module --with-threads --with-http_addition_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_sub_module


####

Clean

- name: Configuring and installing {{ nginx_name }}
  become: yes
  become_user: root
  command:
    argv:
      - ./configure
      - --prefix=/usr/share/nginx
      - --conf-path=/etc/nginx/nginx.conf
      - --lock-path=/var/lock/nginx.lock
      - --pid-path=/run/nginx.pid
      - --modules-path=/usr/lib/nginx/modules
      - --http-client-body-temp-path=/var/lib/nginx/body
      - --http-fastcgi-temp-path=/var/lib/nginx/fastcgi
      - --http-proxy-temp-path=/var/lib/nginx/proxy
      - --http-scgi-temp-path=/var/lib/nginx/scgi
      - --http-uwsgi-temp-path=/var/lib/nginx/uwsgi
      - --http-log-path=/var/log/nginx/access.log
      - --error-log-path=/var/log/nginx/error.log
      - --with-compat
      - --with-debug
      - --with-pcre-jit
      - --with-http_ssl_module
      - --with-http_stub_status_module
      - --with-http_realip_module
      - --with-http_auth_request_module
      - --with-http_v2_module
      - --with-http_dav_module
      - --with-http_slice_module
      - --with-threads
      - --with-http_addition_module
      - --with-http_gunzip_module
      - --with-http_gzip_static_module
      - --with-http_sub_module
      # linker opts can stay together in one arg:
      - "--with-ld-opt=-Wl,-Bsymbolic-functions -flto=auto -Wl,-z,relro -Wl,-z,now -fPIC"
  args:
    chdir: "/tmp/{{ nginx_name }}"
  tags: nginx-build

- name: Compile nginx
  become: yes
  command: make -j"{{ ansible_processor_vcpus | default(2) }}"
  args:
    chdir: "/tmp/{{ nginx_name }}"
  tags: nginx-build

- name: Install nginx
  become: yes
  command: make install
  args:
    chdir: "/tmp/{{ nginx_name }}"
  tags: nginx-build

- name: Verify nginx was built with SSL
  command: /usr/share/nginx/sbin/nginx -V
  register: nginxv
  changed_when: false
  tags: nginx-build

- name: Assert SSL module present
  fail:
    msg: "nginx built WITHOUT SSL (no --with-http_ssl_module in configure args)"
  when: "'--with-http_ssl_module' not in nginxv.stderr"
  tags: nginx-build

####

- name: Backup existing keystore files if present
  tags: 
    - backup_keystores
  block:
    - name: Set backup timestamp
      set_fact:
        backup_ts: "{{ ansible_date_time.iso8601_basic_short }}"

    - name: Define keystore paths
      set_fact:
        drill_dir: "{{ drill_app_dir }}/{{ drill_name }}"
        p12_path: "{{ drill_app_dir }}/{{ drill_name }}/{{ drill_cert_cn }}.p12"
        jks_path: "{{ drill_app_dir }}/{{ drill_name }}/{{ drill_cert_cn }}.jks"
        backup_dir: "{{ drill_app_dir }}/{{ drill_name }}/backup"

    - name: Ensure backup directory exists
      file:
        path: "{{ backup_dir }}"
        state: directory
        mode: "0755"

    - name: Check if p12 exists
      stat:
        path: "{{ p12_path }}"
      register: p12_stat

    - name: Check if jks exists
      stat:
        path: "{{ jks_path }}"
      register: jks_stat

    - name: Backup existing p12
      copy:
        src: "{{ p12_path }}"
        dest: "{{ backup_dir }}/{{ drill_cert_cn }}.p12.{{ backup_ts }}"
        remote_src: yes
        mode: "0644"
      when: p12_stat.stat.exists

    - name: Backup existing jks
      copy:
        src: "{{ jks_path }}"
        dest: "{{ backup_dir }}/{{ drill_cert_cn }}.jks.{{ backup_ts }}"
        remote_src: yes
        mode: "0644"
      when: jks_stat.stat.exists

#####

from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("WindowsToClusterTest") \
    .master("spark://<master-host>:7077") \
    .config("spark.driver.host", "<windows-ip>") \
    .config("spark.driver.bindAddress", "0.0.0.0") \
    .getOrCreate()

data = spark.range(1, 1000000)
print("Count:", data.count())

spark.stop()

####

spark-submit.cmd `
  --master spark://<master-host>:7077 `
  --conf spark.driver.host=<windows-ip> `
  --conf spark.driver.bindAddress=0.0.0.0 `
  C:\Users\Matt\Desktop\test_job.py

  ####

  from pyspark.sql import SparkSession

spark = (SparkSession.builder
         .appName("SanityCheck")
         .master("spark://<MASTER_IP>:7077")
         .config("spark.driver.host","<WINDOWS_IP>")
         .config("spark.driver.bindAddress","0.0.0.0")
         .getOrCreate())

print("Count:", spark.range(1, 1_000_000).count())
spark.stop()


###

set PYSPARK_PYTHON=python
set PYSPARK_DRIVER_PYTHON=python
"C:\spark-3.3.4-bin-hadoop3\bin\spark-submit.cmd" ^
  --master spark://<MASTER_IP>:7077 ^
  --conf spark.driver.host=<WINDOWS_IP> ^
  --conf spark.driver.bindAddress=0.0.0.0 ^
  D:\test_job.py


##

execstart_cmd: "gunicorn -k uvicorn.workers.UvicornWorker --workers 3 --bind=127.0.0.1:{{ gunicorn_port }} manage:app"

##

from fastapi import FastAPI

app = FastAPI()

@app.get("/")
def read_root():
    return {"message": "Hello, FastAPI via Gunicorn + Uvicorn!"}


@app.get("/health")
def health_check():
    return {"status": "ok"}


# Optional: local dev runner
if __name__ == "__main__":
    import uvicorn
    uvicorn.run("manage:app", host="127.0.0.1", port=8000, reload=True)

###

gunicorn -k uvicorn.workers.UvicornH11Worker --workers 1 --bind 127.0.0.1:8000 --access-logfile - --error-logfile - --log-level trace main:app

###

sudo apt install socat -y
sudo systemctl stop py_docragapi.service
sudo socat -v tcp-listen:8000,reuseaddr,fork tcp:127.0.0.1:18000

gunicorn -b 127.0.0.1:18000 main:app

###

--bind unix:/tmp/gunicorn.sock --umask 007

location / {
    proxy_pass http://unix:/tmp/gunicorn.sock:;
    include proxy_params;
}

###

curl -O https://packages.microsoft.com/config/ubuntu/22.04/packages-microsoft-prod.deb


###

https://packages.microsoft.com/ubuntu/22.04/prod/pool/main/m/msodbcsql17/msodbcsql17_17.10.6.1-1_amd64.deb
https://packages.microsoft.com/ubuntu/22.04/prod/pool/main/m/mssql-tools18/mssql-tools18_18.2.1.1-1_amd64.deb

###

sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.10 1
sudo update-alternatives --install /usr/bin/python3 python3 /usr/local/bin/python3.11 2
sudo update-alternatives --config python3

###

/usr/local/bin/python3.11 -c "import sqlite3; print('sqlite3 OK, version:', sqlite3.sqlite_version)"

###

cd /path/to/Python-3.11.x/
sudo make clean
sudo ./configure --enable-optimizations
sudo make altinstall

###

python3 -c "import sqlite3; print(sqlite3.connect(':memory:'))"

###

gunicorn api_request_handler:app -k uvicorn.workers.UvicornWorker --workers 3 --bind 127.0.0.1:{{ gunicorn_port }} --timeout 300 --graceful-timeout 300 --max-requests 200 --max-requests-jitter 20 --log-level debug --access-logfile - --error-logfile -

    proxy_connect_timeout 300s;
    proxy_send_timeout 300s;
    proxy_read_timeout 300s;
    send_timeout 300s;

    proxy_request_buffering off;
    proxy_buffering off;

curl -v \
  -H "Accept: text/event-stream" \
  -H "Content-Type: application/json" \
  --data '{"jsonrpc":"2.0","id":"1","method":"tools.list","params":{}}' \
  https://url/

###

$headers = @{
    "Accept" = "text/event-stream"
    "Content-Type" = "application/json"
}

$body = @{
    "jsonrpc" = "2.0"
    "id"      = "1"
    "method"  = "tools.list"
    "params"  = @{}
} | ConvertTo-Json

Invoke-WebRequest `
    -Uri "https://url" `
    -Headers $headers `
    -Method POST `
    -Body $body `
    -Verbose

###


- block:

    - name: Create Kerberos renew systemd service
      copy:
        dest: /etc/systemd/system/kerb-renew.service
        mode: "0644"
        content: |
          [Unit]
          Description=Renew Kerberos TGT for {{ service_account }}

          [Service]
          Type=oneshot
          User={{ service_account }}
          ExecStart=/usr/bin/kinit -k -t {{ keytab_path }} {{ kerberos_principal }} -c /var/lib/kerberos/krb5cc_{{ service_account }}

    - name: Create Kerberos renew timer
      copy:
        dest: /etc/systemd/system/kerb-renew.timer
        mode: "0644"
        content: |
          [Unit]
          Description=Kerberos ticket renewal timer

          [Timer]
          OnBootSec=5m
          OnUnitActiveSec=1h

          [Install]
          WantedBy=timers.target

    - name: Reload systemd
      systemd:
        daemon_reload: yes

    - name: Enable and start the Kerberos renewal timer
      systemd:
        name: kerb-renew.timer
        enabled: yes
        state: started

  tags:
    - kerberos-renewal

####

nginx -v
which nginx
systemctl status nginx

####

/usr/sbin/nginx -v              # apt version
/usr/local/nginx/sbin/nginx -v # compiled version (if this path exists)

####

cat /lib/systemd/system/nginx.service
# or
cat /etc/systemd/system/nginx.service

####

sudo update-alternatives --remove python3 /usr/local/bin/python3.11
sudo update-alternatives --remove python3 /usr/bin/python3.11
sudo update-alternatives --remove python3 /usr/bin/python3.10

Verify 

update-alternatives --display python3

Restore default

sudo ln -sf /usr/bin/python3.10 /usr/bin/python3

Verify

ls -l /usr/bin/python3
python3 --version
python3 -c "import apt_pkg"

apt update

###

[Unit]
Description=nginx - high performance web server
After=network-online.target
Wants=network-online.target

[Service]
Type=forking
PIDFile=/run/nginx.pid
ExecStartPre=/usr/sbin/nginx -t -q
ExecStart=/usr/sbin/nginx
ExecReload=/usr/sbin/nginx -s reload
ExecStop=/usr/sbin/nginx -s quit
TimeoutStopSec=5
KillMode=process
Restart=on-failure
PrivateTmp=true

[Install]
WantedBy=multi-user.target

####

sudo systemctl stop nginx
sudo systemctl disable nginx
sudo rm -f /etc/systemd/system/nginx.service
sudo systemctl daemon-reload
sudo rm -f /usr/sbin/nginx
sudo rm -rf /usr/share/nginx
sudo rm -rf /usr/lib/nginx


###

systemctl stop nginx || true && systemctl disable nginx || true && systemctl unmask nginx || true && rm -f /etc/systemd/system/nginx.service && systemctl daemon-reload && rm -f /usr/sbin/nginx && rm -rf /usr/share/nginx /usr/lib/nginx /var/lib/nginx

nginx -V 2>&1 | tr ' ' '\n' | grep -- '--with'

###

count objects in bad prefix

aws s3 ls s3://PROBLEM_BUCKET/PROBLEM_PREFIX/ --recursive --summarize

###

# ============================================================
# S3 OVERWRITE / METADATA STRESS TEST (PowerShell)
#
# PURPOSE:
#   Demonstrate how LIST and DELETE operations degrade as
#   object count grows, even when PUT (create) is stable.
#
#
# SAFE USE:
#   - ONLY run this in a TEST bucket / TEST prefix
#   - This script DELETES objects
#
# ============================================================


# ----------------------------
# CONFIGURATION (EDIT THESE)
# ----------------------------

$Bucket  = "my-test-bucket"          # Test bucket ONLY
$Prefix  = "overwrite-test/data"     # Test prefix ONLY
$Objects = 5000                      # Number of objects to create
$Cycles  = 3                         # Number of overwrite cycles


# ----------------------------
# HELPER FUNCTION
# ----------------------------

function Time-Step {
    param (
        [string]$Name,
        [scriptblock]$Action
    )

    Write-Host "--------------------------------------------------"
    Write-Host $Name
    Write-Host "--------------------------------------------------"

    $elapsed = Measure-Command { & $Action }

    Write-Host ("Time taken: {0:N2} seconds" -f $elapsed.TotalSeconds)
    Write-Host ""
}


# ----------------------------
# STEP 0 ‚Äî CLEAN START
# Ensure prefix is empty
# ----------------------------

Write-Host "Cleaning test prefix to ensure a fresh start..."
aws s3 rm "s3://$Bucket/$Prefix/" --recursive | Out-Null
Write-Host "Prefix cleaned."
Write-Host ""


# ----------------------------
# STEP 1 ‚Äî INITIAL CREATE (PUT)
# Simulates Spark writing data
# ----------------------------

Time-Step "CREATE: Uploading $Objects objects (PUT baseline)" {

    for ($i = 1; $i -le $Objects; $i++) {
        aws s3 cp NUL "s3://$Bucket/$Prefix/file-$i" | Out-Null
    }
}


# ----------------------------
# STEP 2 ‚Äî LIST OBJECTS
# Simulates Spark enumerating existing data
# ----------------------------

Time-Step "LIST: Enumerating objects under prefix" {

    aws s3 ls "s3://$Bucket/$Prefix/" --recursive | Out-Null
}


# ----------------------------
# STEP 3 ‚Äî DELETE OBJECTS
# Simulates Spark OVERWRITE cleanup
# ----------------------------

Time-Step "DELETE: Removing all objects (overwrite cleanup)" {

    aws s3 rm "s3://$Bucket/$Prefix/" --recursive | Out-Null
}


# ----------------------------
# STEP 4 ‚Äî REPEAT OVERWRITE CYCLES
# Shows degradation over time
# ----------------------------

for ($cycle = 1; $cycle -le $Cycles; $cycle++) {

    Write-Host "================ OVERWRITE CYCLE $cycle ================"

    Time-Step "RE-CREATE: Uploading $Objects objects" {

        for ($i = 1; $i -le $Objects; $i++) {
            aws s3 cp NUL "s3://$Bucket/$Prefix/file-$i" | Out-Null
        }
    }

    Time-Step "DELETE: Removing objects (overwrite cleanup)" {

        aws s3 rm "s3://$Bucket/$Prefix/" --recursive | Out-Null
    }
}

Write-Host "=================================================="
Write-Host "TEST COMPLETE"
Write-Host "=================================================="

###

- name: Stop nginx service
  ansible.builtin.service:
    name: nginx
    state: stopped
  become: true

- name: Remove nginx packages installed via dnf
  ansible.builtin.dnf:
    name:
      - nginx
      - nginx-core
      - nginx-mod*
    state: absent
  become: true

####


- name: Create unique archive
  ansible.builtin.archive:
    path: /data/d/d
    dest: "/apps/backups/dss_data_{{ ansible_date_time.iso8601_basic }}.tar.gz"
    format: gz
  become: true

===

python -c "import os,requests; os.environ['REQUESTS_CA_BUNDLE']='/path/to/your/internal-bundle.pem'; os.environ['SSL_CERT_FILE']='/path/to/your/internal-bundle.pem'; print(requests.get('https://your.internal.endpoint', timeout=10).status_code)"

====

python -c "import requests; print(requests.get('https://your.internal.endpoint', timeout=10).status_code)"

####

cat > /tmp/node_disk_snapshot.sh <<'EOF'
#!/usr/bin/env bash
set -euo pipefail

OUT_DIR="${1:-/tmp}"
TS="$(date +%F_%H%M%S)"
HOST="$(hostname -f 2>/dev/null || hostname)"
OUT="${OUT_DIR%/}/disk_snapshot_${HOST}_${TS}.txt"

run() {
  echo
  echo "################################################################"
  echo "# $*"
  echo "################################################################"
  bash -lc "$*" 2>&1 || echo "[non-zero exit: $?]"
}

{
  echo "Snapshot file: $OUT"
  echo "Host: $HOST"
  echo "Date: $(date -Is)"
  echo "Uptime: $(uptime -p 2>/dev/null || true)"
  echo "Kernel: $(uname -a)"
  echo "OS: $(. /etc/os-release 2>/dev/null; echo ${PRETTY_NAME:-unknown})"
  echo "==============================================================="
} > "$OUT"

# Identity / time (helps correlate with tickets)
{
  run "whoami"
  run "id"
  run "timedatectl 2>/dev/null || true"
} >> "$OUT"

# Disk / storage topology + mounts
{
  run "lsblk -o NAME,MAJ:MIN,SIZE,ROTA,TYPE,FSTYPE,MOUNTPOINT,MODEL"
  run "lsblk -f"
  run "lsblk -t"
  run "blkid 2>/dev/null || true"
  run "df -hT"
  run "df -i"
  run "mount"
  run "findmnt -o TARGET,SOURCE,FSTYPE,OPTIONS"
  run "cat /etc/fstab 2>/dev/null || true"
} >> "$OUT"

# Kernel / driver / virtualization hints
{
  run "systemd-detect-virt 2>/dev/null || true"
  run "lscpu | sed -n '1,120p'"
  run "lsmod | grep -iE 'virtio|scsi|nvme|blk' || true"
  run "dmesg -T | grep -iE 'virtio|scsi|nvme|blk' | tail -200 || true"
} >> "$OUT"

# Queue / scheduler parameters (important for I/O behavior)
{
  run "echo '--- /sys/block/*/queue/scheduler ---'; for f in /sys/block/*/queue/scheduler; do echo \"$f: $(cat \"$f\")\"; done"
  run "echo '--- /sys/block/*/queue/nr_requests ---'; for f in /sys/block/*/queue/nr_requests; do echo \"$f: $(cat \"$f\")\"; done"
  run "echo '--- /sys/block/*/queue/read_ahead_kb ---'; for f in /sys/block/*/queue/read_ahead_kb; do echo \"$f: $(cat \"$f\")\"; done"
  run "echo '--- /sys/block/*/queue/max_sectors_kb ---'; for f in /sys/block/*/queue/max_sectors_kb; do echo \"$f: $(cat \"$f\")\"; done"
  run "echo '--- /sys/block/*/queue/queue_depth (if present) ---'; for f in /sys/block/*/queue/queue_depth; do echo \"$f: $(cat \"$f\" 2>/dev/null || true)\"; done"
  run "echo '--- /sys/block/*/queue/rotational ---'; for f in /sys/block/*/queue/rotational; do echo \"$f: $(cat \"$f\")\"; done"
} >> "$OUT"

# I/O-related warnings from kernel logs (high signal)
{
  run "dmesg -T | grep -iE 'I/O error|timeout|reset|hung task|blocked for more than|Buffer I/O|EXT4-fs error|xfs error|nvme.*reset|blk_update_request' || true"
  run "journalctl -k --no-pager -n 400 2>/dev/null || true"
} >> "$OUT"

# Quick live I/O snapshot (non-intrusive)
{
  run "command -v iostat >/dev/null && iostat -xz 1 10 || echo 'iostat not installed'"
  run "vmstat 1 10"
  run "command -v pidstat >/dev/null && pidstat -d 1 5 || echo 'pidstat not installed'"
} >> "$OUT"

# Useful sysctls that affect I/O / VM behavior (read-only)
{
  run "sysctl vm.dirty_background_ratio vm.dirty_ratio vm.dirty_expire_centisecs vm.dirty_writeback_centisecs 2>/dev/null || true"
  run "sysctl fs.file-max fs.inotify.max_user_watches 2>/dev/null || true"
} >> "$OUT"

echo "Wrote: $OUT"
EOF

chmod +x /tmp/node_disk_snapshot.sh

###

sudo /tmp/node_disk_snapshot.sh /tmp

###

- name: Upgrade nginx with tagged rollback support
  hosts: nginx_servers
  become: true

  vars:
    nginx_new_version: "1.26.2"
    nginx_old_version: "1.20.1"
    rhel_major: "8"
    rpm_dir: "/tmp/nginx-rpms"

  tasks:
    - name: Create temp directory for nginx RPMs
      tags: [nginx_upgrade, nginx_prep_dir]
      file:
        path: "{{ rpm_dir }}"
        state: directory
        mode: "0755"

    - name: Download nginx-filesystem RPM
      tags: [nginx_upgrade, nginx_download_filesystem]
      get_url:
        url: "https://nginx.org/packages/rhel/{{ rhel_major }}/x86_64/nginx-filesystem-{{ nginx_new_version }}-1.el{{ rhel_major }}.ngx.noarch.rpm"
        dest: "{{ rpm_dir }}/"
        mode: "0644"

    - name: Download nginx RPM
      tags: [nginx_upgrade, nginx_download_binary]
      get_url:
        url: "https://nginx.org/packages/rhel/{{ rhel_major }}/x86_64/nginx-{{ nginx_new_version }}-1.el{{ rhel_major }}.ngx.x86_64.rpm"
        dest: "{{ rpm_dir }}/"
        mode: "0644"

    - name: Disable RHEL nginx module (RHEL 8/9 only)
      tags: [nginx_upgrade, nginx_disable_module]
      command: dnf -y module disable nginx
      when: ansible_distribution_major_version | int >= 8
      changed_when: false

    - block:

        - name: Upgrade nginx in place
          tags: [nginx_upgrade, nginx_install]
          dnf:
            name:
              - "{{ rpm_dir }}/nginx-filesystem-*.rpm"
              - "{{ rpm_dir }}/nginx-*.rpm"
            state: present
            disable_gpg_check: true

        - name: Validate nginx configuration
          tags: [nginx_upgrade, nginx_validate]
          command: nginx -t
          changed_when: false

        - name: Restart nginx
          tags: [nginx_upgrade, nginx_restart]
          service:
            name: nginx
            state: restarted
            enabled: true

      rescue:

        - name: Roll back nginx to previous version
          tags: [nginx_rollback, nginx_rollback_install]
          dnf:
            name: "nginx-{{ nginx_old_version }}*"
            state: present
            allow_downgrade: true

        - name: Restart nginx after rollback
          tags: [nginx_rollback, nginx_rollback_restart]
          service:
            name: nginx
            state: restarted

        - name: Fail play after rollback
          tags: [nginx_rollback, nginx_rollback_fail]
          fail:
            msg: >
              NGINX upgrade failed.
              Rolled back to version {{ nginx_old_version }}.
              Check nginx config and logs.

###

from py4j.java_gateway import java_import

jvm = spark.sparkContext._jvm
Path = jvm.org.apache.hadoop.fs.Path

p = Path("s3a://buck/test")
u = p.toUri()

print("URI:", u.toString())
print("host:", u.getHost())
print("path:", u.getPath())

###

Spark accesses S3 via Hadoop‚Äôs S3A connector, which parses paths like s3a://bucket/key by treating the bucket name as the URI host. The target ECS bucket contains underscores, and Hadoop‚Äôs URI parser doesn‚Äôt accept underscores as a valid host, so the bucket isn‚Äôt recognised during parsing. As a result S3A receives a null/empty bucket and fails with IllegalArgumentException: bucket is null/empty. Tools like AWS CLI work because they don‚Äôt rely on Hadoop URI parsing. The practical fix is to mirror or expose the data in a bucket with a compliant name (lowercase, no underscores) for Spark to read.